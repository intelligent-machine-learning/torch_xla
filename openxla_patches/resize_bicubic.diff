diff --git a/xla/debug_options_flags.cc b/xla/debug_options_flags.cc
index 440014453..7c53826fd 100644
--- a/xla/debug_options_flags.cc
+++ b/xla/debug_options_flags.cc
@@ -794,7 +794,7 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,
       "this flag to false."));
   flag_list->push_back(tsl::Flag(
       "xla_multiheap_size_constraint_per_heap",
-      int32_setter_for(
+      int64_setter_for(
           &DebugOptions::set_xla_multiheap_size_constraint_per_heap),
       debug_options->xla_multiheap_size_constraint_per_heap(),
       "Generates multiple heaps (i.e., temp buffers) with a size "
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index 449b43cb6..52ebc0bce 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -72,7 +72,8 @@ void EnablePeerAccess(absl::Span<se::StreamExecutor* const> executors) {
 
 // Builds a BFCAllocator for all local GPUs.
 StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
-    se::StreamExecutor* executor, double memory_fraction, bool preallocate) {
+    se::StreamExecutor* executor, double memory_fraction, bool preallocate,
+    bool garbage_collection) {
   bool enable_unified_memory;
   Status status = tsl::ReadBoolFromEnvVar("TF_FORCE_UNIFIED_MEMORY", false,
                                           &enable_unified_memory);
@@ -111,6 +112,7 @@ StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
 
   tsl::BFCAllocator::Options opts;
   opts.allow_growth = !preallocate;
+  opts.garbage_collection = garbage_collection;
   return std::make_unique<tsl::BFCAllocator>(
       std::move(sub_allocator), allocator_memory,
       absl::StrCat("GPU_", device_ordinal, "_bfc"), opts);
diff --git a/xla/pjrt/gpu/gpu_helpers.h b/xla/pjrt/gpu/gpu_helpers.h
index 0549e488d..3b3d38e5f 100644
--- a/xla/pjrt/gpu/gpu_helpers.h
+++ b/xla/pjrt/gpu/gpu_helpers.h
@@ -57,6 +57,9 @@ struct GpuAllocatorConfig {
   // fragmentation, allowing more of the total memory to be used. If false, the
   // allocator will allocate more memory as allocations are requested.
   bool preallocate = true;
+
+  // activate garbage collection or not
+  bool garbage_collection = false;
 };
 
 std::unique_ptr<tsl::BFCAllocator> GetGpuHostAllocator(
@@ -64,7 +67,8 @@ std::unique_ptr<tsl::BFCAllocator> GetGpuHostAllocator(
 
 // Builds a BFCAllocator for all local GPUs.
 StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
-    se::StreamExecutor* executor, double memory_fraction, bool preallocate);
+    se::StreamExecutor* executor, double memory_fraction, bool preallocate,
+    bool garbage_collection);
 
 }  // namespace xla
 
diff --git a/xla/pjrt/gpu/se_gpu_pjrt_client.cc b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
index 43aabbb9c..595e92437 100644
--- a/xla/pjrt/gpu/se_gpu_pjrt_client.cc
+++ b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
@@ -745,7 +745,8 @@ GetStreamExecutorGpuDeviceAllocator(
             auto bfc_allocator,
             CreateBFCAllocator(ordinal_and_device.second->executor(),
                                allocator_config.memory_fraction,
-                               allocator_config.preallocate));
+                               allocator_config.preallocate,
+                               allocator_config.garbage_collection));
         allocators_and_streams.emplace_back(
             std::move(bfc_allocator),
             ordinal_and_device.second->compute_stream());
diff --git a/xla/service/buffer_assignment.cc b/xla/service/buffer_assignment.cc
index 3235e5d1d..cbe909220 100644
--- a/xla/service/buffer_assignment.cc
+++ b/xla/service/buffer_assignment.cc
@@ -2019,7 +2019,7 @@ StatusOr<std::unique_ptr<BufferAssignment>> BufferAssigner::CreateAssignment(
       buffers_to_assign_sequentially.size() == global_computations.size();
   VLOG(2) << "Running whole module heap simulation: "
           << run_whole_module_heap_simulation;
-  const int32_t multiheap_size_constraint_per_heap =
+  const int64_t multiheap_size_constraint_per_heap =
       module->config().debug_options().xla_multiheap_size_constraint_per_heap();
   VLOG(2) << "Multiheap per heap size limit: "
           << multiheap_size_constraint_per_heap;
diff --git a/xla/service/buffer_assignment.h b/xla/service/buffer_assignment.h
index 0c8ebed0d..deab104da 100644
--- a/xla/service/buffer_assignment.h
+++ b/xla/service/buffer_assignment.h
@@ -530,7 +530,7 @@ class BufferAssignment {
         color_alignment_(std::move(color_alignment)),
         alias_analysis_(std::move(alias_analysis)),
         hlo_live_range_(std::move(hlo_live_range)) {
-    int32_t raw_value = module->config()
+    int64_t raw_value = module->config()
                             .debug_options()
                             .xla_multiheap_size_constraint_per_heap();
     // -1 means no constraint.
diff --git a/xla/service/gpu/runtime/BUILD b/xla/service/gpu/runtime/BUILD
index ef48fdd16..5ce7e7e1c 100644
--- a/xla/service/gpu/runtime/BUILD
+++ b/xla/service/gpu/runtime/BUILD
@@ -257,6 +257,7 @@ cc_library(
         ":stream_synchronization",
         ":support",
         ":topk",
+        ":resize_bicubic",
         ":tracing",
         "//xla:statusor",
         "//xla:xla_proto_cc",
@@ -417,6 +418,85 @@ cc_library(
     ],
 )
 
+cc_library(
+    name = "resize_bicubic_kernel",
+    srcs = if_cuda_is_configured(
+        [
+            "resize_bicubic_kernel.cc",
+        ],
+    ),
+    hdrs = if_cuda_is_configured(["resize_bicubic_kernel.h"]),
+    compatible_with = [],
+    deps = [
+        ":resize_bicubic_kernel_cuda",
+        # "//xla:shape_util",
+        "//xla:xla_proto_cc",
+        "//xla:xla_data_proto_cc",
+        "//xla/runtime:memref_view",
+        "//xla/stream_executor:platform",
+        "//xla/stream_executor:stream_executor_headers",  # build_cleaner: keep
+        "//xla/stream_executor/gpu:gpu_stream_header",
+        "//xla/stream_executor/gpu:gpu_types_header",
+        "@com_google_absl//absl/numeric:bits",
+        "@com_google_absl//absl/status",
+        "@com_google_absl//absl/status:statusor",
+        "@local_config_cuda//cuda:cuda_headers",
+    ],
+)
+
+cuda_library(
+    name = "resize_bicubic_kernel_cuda",
+    srcs = if_cuda_is_configured(
+        [
+            "resize_bicubic_kernel.cu.cc",
+        ],
+    ),
+    hdrs = if_cuda_is_configured(["resize_bicubic_kernel_common.h"]),
+    compatible_with = [],
+    deps = [
+        "@eigen_archive//:eigen3",
+        "@local_config_cuda//cuda:cuda_headers",
+        "@com_google_absl//absl/types:span",
+    ],
+)
+
+
+cc_library(
+    name = "resize_bicubic",
+    srcs = if_cuda_is_configured(
+        ["resize_bicubic.cc"],
+    ),
+    hdrs = ["resize_bicubic.h"],
+    deps = if_cuda_is_configured([":resize_bicubic_kernel"]) + [
+        ":support",
+        "//xla:executable_run_options",
+        # "//xla:shape_util",
+        "//xla:status",
+        "//xla:statusor",
+        # "//xla:types",
+        "//xla:xla_data_proto_cc",
+        "//xla:xla_proto_cc",
+        "//xla/hlo/ir:hlo",
+        # "//xla/mlir/runtime/transforms:custom_call_encoding",
+        "//xla/runtime:custom_call",
+        "//xla/runtime:custom_call_registry",
+        "//xla/runtime:executable",
+        "//xla/runtime:state",
+        # "//xla/runtime/ffi:ffi_api",
+        # "//xla/runtime/ffi:ffi_c_api_hdrs",
+        "//xla/service:executable",
+        "//xla/service:hlo_pass",
+        "//xla/service:tuple_util",
+        "//xla/stream_executor/gpu:gpu_stream_header",
+        "//xla/stream_executor/gpu:gpu_types_header",
+        "@com_google_absl//absl/container:flat_hash_set",
+        "@com_google_absl//absl/log:check",
+        "@com_google_absl//absl/status",
+        "@com_google_absl//absl/strings",
+        "@tsl//tsl/platform:statusor",
+    ],
+)
+
 cc_library(
     name = "gemm",
     srcs = ["gemm.cc"],
diff --git a/xla/service/gpu/runtime/executable.cc b/xla/service/gpu/runtime/executable.cc
index f75fe6510..b2eed355a 100644
--- a/xla/service/gpu/runtime/executable.cc
+++ b/xla/service/gpu/runtime/executable.cc
@@ -47,6 +47,7 @@ limitations under the License.
 #include "xla/service/gpu/runtime/stream_synchronization.h"
 #include "xla/service/gpu/runtime/support.h"
 #include "xla/service/gpu/runtime/topk.h"
+#include "xla/service/gpu/runtime/resize_bicubic.h"
 #include "xla/service/gpu/runtime/tracing.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/service_executable_run_options.h"
@@ -87,6 +88,7 @@ void RegisterXlaGpuRuntimeCustomCalls(DirectCustomCallRegistry& registry) {
   RegisterMemsetCustomCalls(registry);
   RegisterSendRecvCustomCalls(registry);
   RegisterTopkCustomCall(registry);
+  RegisterResizeBicubicCustomCall(registry);
 
 #if GOOGLE_CUDA || TF_HIPBLASLT
   RegisterMatmulCustomCalls(registry);
diff --git a/xla/service/gpu/runtime/resize_bicubic.cc b/xla/service/gpu/runtime/resize_bicubic.cc
new file mode 100644
index 000000000..22bd7faa6
--- /dev/null
+++ b/xla/service/gpu/runtime/resize_bicubic.cc
@@ -0,0 +1,88 @@
+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/runtime/resize_bicubic.h"
+
+#include <stdint.h>
+
+#include <cstddef>
+
+#include "absl/status/status.h"
+#include "absl/types/span.h"
+#include "xla/runtime/custom_call.h"
+#include "xla/runtime/executable.h"
+#include "xla/service/gpu/runtime/resize_bicubic_kernel.h"
+// #include "xla/runtime/custom_call_registry.h"
+
+#include "xla/service/gpu/runtime/support.h"
+#include "xla/service/service_executable_run_options.h"
+#include "xla/xla_data.pb.h"
+
+namespace xla::gpu {
+using ::xla::runtime::CustomCall;
+using ::xla::runtime::StridedMemrefView;
+
+static absl::Status ResizeBicubicImpl(
+    const ServiceExecutableRunOptions* run_options, StridedMemrefView input,
+    StridedMemrefView output, bool align_corners) {
+  float scales_h =
+      static_cast<float>(output.sizes[2]) / static_cast<float>(input.sizes[2]);
+  float scales_w =
+      static_cast<float>(output.sizes[3]) / static_cast<float>(input.sizes[3]);
+  se::StreamExecutor* executor = run_options->stream()->parent();
+  return RunResizeBicubicImpl(
+      se::gpu::AsGpuStreamValue(run_options->stream()),
+      executor->GetDeviceDescription().threads_per_block_limit(), input, output,
+      align_corners, scales_h, scales_w);
+}
+
+static absl::Status ResizeBicubicGradImpl(
+    const ServiceExecutableRunOptions* run_options,
+    StridedMemrefView grad_output, StridedMemrefView grad_input,
+    bool align_corners) {
+  float scales_h = static_cast<float>(grad_output.sizes[2]) /
+                   static_cast<float>(grad_input.sizes[2]);
+  float scales_w = static_cast<float>(grad_output.sizes[3]) /
+                   static_cast<float>(grad_input.sizes[3]);
+  se::StreamExecutor* executor = run_options->stream()->parent();
+  return RunResizeBicubicGradImpl(
+      se::gpu::AsGpuStreamValue(run_options->stream()),
+      executor->GetDeviceDescription().threads_per_block_limit(), grad_input,
+      grad_output, align_corners, scales_h, scales_w);
+}
+
+XLA_RUNTIME_DEFINE_CUSTOM_CALL(
+    ResizeBicubic, FunctionWrapper<ResizeBicubicImpl>(), checks,
+    CustomCall::Bind("__gpu$ResizeBicubic")
+        .UserData<const ServiceExecutableRunOptions*>()
+        .Arg<StridedMemrefView>()  // input
+        .Arg<StridedMemrefView>()  // output
+        .Attr<bool>("align_corners"));
+
+XLA_RUNTIME_DEFINE_CUSTOM_CALL(
+    ResizeBicubicGrad, FunctionWrapper<ResizeBicubicGradImpl>(), checks,
+    CustomCall::Bind("__gpu$ResizeBicubicGrad")
+        .UserData<const ServiceExecutableRunOptions*>()
+        .Arg<StridedMemrefView>()  // grad_output
+        .Arg<StridedMemrefView>()  // grad_input
+        .Attr<bool>("align_corners"));
+
+void RegisterResizeBicubicCustomCall(
+    runtime::DirectCustomCallRegistry& registry) {
+  registry.Register("__gpu$ResizeBicubic", ResizeBicubic);
+  registry.Register("__gpu$ResizeBicubicGrad", ResizeBicubicGrad);
+}
+
+}  // namespace xla::gpu
diff --git a/xla/service/gpu/runtime/resize_bicubic.h b/xla/service/gpu/runtime/resize_bicubic.h
new file mode 100644
index 000000000..6ded78e38
--- /dev/null
+++ b/xla/service/gpu/runtime/resize_bicubic.h
@@ -0,0 +1,28 @@
+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_H_
+#define XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_H_
+
+#include "xla/runtime/custom_call_registry.h"
+
+namespace xla::gpu {
+
+// Registers XLA Gpu runtime TopK custom calls.
+void RegisterResizeBicubicCustomCall(runtime::DirectCustomCallRegistry& registry);
+
+}  // namespace xla::gpu
+
+#endif  // XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_H_
diff --git a/xla/service/gpu/runtime/resize_bicubic_kernel.cc b/xla/service/gpu/runtime/resize_bicubic_kernel.cc
new file mode 100644
index 000000000..c596682b3
--- /dev/null
+++ b/xla/service/gpu/runtime/resize_bicubic_kernel.cc
@@ -0,0 +1,197 @@
+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/xla_data.pb.h"
+#include "xla/service/gpu/runtime/resize_bicubic_kernel.h"
+
+#include <algorithm>
+
+#include "absl/numeric/bits.h"
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "third_party/gpus/cuda/include/cuda_runtime_api.h"
+#include "xla/service/gpu/runtime/resize_bicubic_kernel_common.h"
+
+namespace xla::gpu {
+using ::stream_executor::gpu::GpuStreamHandle;
+
+float compute_scales_value(double scale, int64_t src_size, int64_t dst_size) {
+  return (scale > 0.) ? (float)(1.0 / scale) : (float)src_size / dst_size;
+}
+
+float area_pixel_compute_scale(int input_size, int output_size,
+                               bool align_corners, double scale) {
+  if (align_corners) {
+    if (output_size > 1) {
+      return (float)(input_size - 1) / (output_size - 1);
+    } else {
+      return static_cast<float>(0);
+    }
+  } else {
+    return compute_scales_value(scale, input_size, output_size);
+  }
+}
+
+absl::Status RunResizeBicubicImpl(::tensorflow::se::gpu::GpuStreamHandle stream,
+                                  int threads_per_block_limit,
+                                  xla::runtime::StridedMemrefView input,
+                                  xla::runtime::StridedMemrefView output,
+                                  bool align_corners, float scales_h,
+                                  float scales_w) {
+  int output_height = output.sizes[2];
+  int output_width = output.sizes[3];
+
+  int input_height = input.sizes[2];
+  int input_width = input.sizes[3];
+
+  int num_output_elements = output_height * output_width;
+  int threads_per_block = std::min(threads_per_block_limit, 1024);
+  int blocks_per_grid =
+      (num_output_elements + threads_per_block - 1) / threads_per_block;
+  int shmem_size = 0;
+
+  // Get scaling factors
+  float rheight = area_pixel_compute_scale(input_height, output_height,
+                                           align_corners, scales_h);
+  float rwidth = area_pixel_compute_scale(input_width, output_width,
+                                          align_corners, scales_w);
+  void* kernel = nullptr;
+  // kernel = GetResizeBicubicKernel<float>();
+  xla::PrimitiveType dtype = static_cast<xla::PrimitiveType>(input.dtype);
+  switch (dtype) {
+    case xla::PrimitiveType::F32:
+      kernel = GetResizeBicubicKernel<float>();
+      break;
+
+    case xla::PrimitiveType::BF16:
+      kernel = GetResizeBicubicKernel<Eigen::bfloat16>();
+      break;
+
+    case xla::PrimitiveType::F16:
+      kernel = GetResizeBicubicKernel<Eigen::half>();
+      break;
+
+    default:
+      return absl::UnimplementedError(
+          absl::StrCat("ResizeBicubic not implemented for this dtype: ", dtype,
+                       " PrimitiveType::F32: ", xla::PrimitiveType::F32,
+                       "equals ", xla::PrimitiveType::F32 == dtype));
+  }
+
+  absl::Span<const int64_t>* isizes = &input.sizes;
+  absl::Span<const int64_t>* osizes = &output.sizes;
+  int batchsize = (*isizes)[0];
+  int channels = (*isizes)[1];
+  int64_t istrides0 = input.strides[0];
+  int64_t istrides1 = input.strides[1];
+  int64_t istrides2 = input.strides[2];
+  int64_t istrides3 = input.strides[3];
+  int64_t ostrides0 = output.strides[0];
+  int64_t ostrides1 = output.strides[1];
+  int64_t ostrides2 = output.strides[2];
+  int64_t ostrides3 = output.strides[3];
+  void* kernel_args[] = {&num_output_elements, &batchsize,   &channels,
+                         &input_height,        &input_width, &output_height,
+                         &output_width,        &rheight,     &rwidth,
+                         &align_corners,       &input.data,  &output.data,
+                         &istrides0,           &istrides1,   &istrides2,
+                         &istrides3,           &ostrides0,   &ostrides1,
+                         &ostrides2,           &ostrides3};
+  cudaError_t launch_status =
+      cudaLaunchKernel(kernel, blocks_per_grid, threads_per_block, kernel_args,
+                       shmem_size, stream);
+  if (launch_status != cudaSuccess) {
+    return absl::InternalError(absl::StrCat("Failed to launch kernel: ",
+                                            cudaGetErrorString(launch_status)));
+  }
+  return absl::OkStatus();
+}
+
+absl::Status RunResizeBicubicGradImpl(
+    ::tensorflow::se::gpu::GpuStreamHandle stream, int threads_per_block_limit,
+    xla::runtime::StridedMemrefView grad_input,
+    xla::runtime::StridedMemrefView grad_output, bool align_corners,
+    float scales_h, float scales_w) {
+  int output_height = grad_output.sizes[2];
+  int output_width = grad_output.sizes[3];
+
+  int input_height = grad_input.sizes[2];
+  int input_width = grad_input.sizes[3];
+  int num_output_elements = output_height * output_width;
+  int threads_per_block = std::min(threads_per_block_limit, 1024);
+  int blocks_per_grid =
+      (num_output_elements + threads_per_block - 1) / threads_per_block;
+  int shmem_size = 0;
+
+  // Get scaling factors
+  float rheight = area_pixel_compute_scale(input_height, output_height,
+                                           align_corners, scales_h);
+  float rwidth = area_pixel_compute_scale(input_width, output_width,
+                                          align_corners, scales_w);
+
+  void* kernel = nullptr;
+  // kernel = GetResizeBicubicGradKernel<float>();
+  xla::PrimitiveType dtype = static_cast<xla::PrimitiveType>(grad_input.dtype);
+  switch (dtype) {
+    case xla::PrimitiveType::F32:
+      kernel = GetResizeBicubicGradKernel<float>();
+      break;
+
+    case xla::PrimitiveType::BF16:
+      kernel = GetResizeBicubicGradKernel<Eigen::bfloat16>();
+      break;
+
+    case xla::PrimitiveType::F16:
+      kernel = GetResizeBicubicGradKernel<Eigen::half>();
+      break;
+
+    default:
+      return absl::UnimplementedError(absl::StrCat(
+          "ResizeBicubicGrad not implemented for this dtype: ", dtype,
+          " PrimitiveType::F32: ", xla::PrimitiveType::F32, "equal",
+          dtype == xla::PrimitiveType::F32));
+  }
+
+  absl::Span<const int64_t>* isizes = &grad_input.sizes;
+  absl::Span<const int64_t>* osizes = &grad_output.sizes;
+  int batchsize = (*isizes)[0];
+  int channels = (*isizes)[1];
+  int64_t istrides0 = grad_input.strides[0];
+  int64_t istrides1 = grad_input.strides[1];
+  int64_t istrides2 = grad_input.strides[2];
+  int64_t istrides3 = grad_input.strides[3];
+  int64_t ostrides0 = grad_output.strides[0];
+  int64_t ostrides1 = grad_output.strides[1];
+  int64_t ostrides2 = grad_output.strides[2];
+  int64_t ostrides3 = grad_output.strides[3];
+  void* kernel_args[] = {
+      &num_output_elements, &batchsize,       &channels,
+      &input_height,        &input_width,     &output_height,
+      &output_width,        &rheight,         &rwidth,
+      &align_corners,       &grad_input.data, &grad_output.data,
+      &istrides0,           &istrides1,       &istrides2,
+      &istrides3,           &ostrides0,       &ostrides1,
+      &ostrides2,           &ostrides3};
+  cudaError_t launch_status =
+      cudaLaunchKernel(kernel, blocks_per_grid, threads_per_block, kernel_args,
+                       shmem_size, stream);
+  if (launch_status != cudaSuccess) {
+    return absl::InternalError(absl::StrCat("Failed to launch kernel: ",
+                                            cudaGetErrorString(launch_status)));
+  }
+  return absl::OkStatus();
+}
+
+}  // namespace xla::gpu
diff --git a/xla/service/gpu/runtime/resize_bicubic_kernel.cu.cc b/xla/service/gpu/runtime/resize_bicubic_kernel.cu.cc
new file mode 100644
index 000000000..95118f639
--- /dev/null
+++ b/xla/service/gpu/runtime/resize_bicubic_kernel.cu.cc
@@ -0,0 +1,323 @@
+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include <cuda_bf16.h>
+#include <cuda_fp16.h>
+#include <stdio.h>
+
+#include <cstddef>
+#include <cstdint>
+#include <limits>
+
+#include "Eigen/Core"  // from @eigen_archive
+#include "absl/types/span.h"
+#include "xla/service/gpu/runtime/resize_bicubic_kernel_common.h"
+
+namespace xla::gpu {
+namespace {
+
+#define C10_MAX_THREADS_PER_BLOCK(val)           \
+  (((val) <= CUDA_MAX_THREADS_PER_BLOCK) ? (val) \
+                                         : CUDA_THREADS_PER_BLOCK_FALLBACK)
+#define C10_LAUNCH_BOUNDS_1(max_threads_per_block) \
+  __launch_bounds__((C10_MAX_THREADS_PER_BLOCK((max_threads_per_block))))
+
+template <typename accscalar_t>
+__device__ __forceinline__ static accscalar_t area_pixel_compute_source_index(
+    accscalar_t scale, int dst_index, bool align_corners, bool cubic) {
+  if (align_corners) {
+    return scale * dst_index;
+  } else {
+    accscalar_t src_idx = scale * (dst_index + static_cast<accscalar_t>(0.5)) -
+                          static_cast<accscalar_t>(0.5);
+    // See Note[Follow Opencv resize logic]
+    return (!cubic && src_idx < static_cast<accscalar_t>(0))
+               ? static_cast<accscalar_t>(0)
+               : src_idx;
+  }
+}
+
+template <typename scalar_t>
+__device__ __forceinline__ static scalar_t upsample_get_value_bounded(
+    const scalar_t* data, int batch, int channel, int height, int width, int y,
+    int x, int64_t strides0, int64_t strides1, int64_t strides2,
+    int64_t strides3) {
+  int access_y = max(min(y, height - 1), 0);
+  int access_x = max(min(x, width - 1), 0);
+  const int64_t offset = batch * strides0 + channel * strides1 +
+                         access_y * strides2 + access_x * strides3;
+  return data[offset];
+}
+
+static inline __device__ void gpuAtomicAddNoRet(float* address, float val) {
+  atomicAdd(address, val);
+}
+
+#if defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)))
+static inline __device__ void gpuAtomicAddNoRet(Eigen::half* address,
+                                                Eigen::half val) {
+  unsigned int* address_as_ui =
+      (unsigned int*)((char*)address - ((size_t)address & 2));
+  unsigned int old = *address_as_ui;
+  unsigned int assumed;
+  do {
+    assumed = old;
+    uint16_t hsum = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
+    hsum += val;
+    old = (size_t)address & 2 ? (old & 0xffff) | (hsum << 16)
+                              : (old & 0xffff0000) | hsum;
+    old = atomicCAS(address_as_ui, assumed, old);
+  } while (assumed != old);
+}
+#else
+static inline __device__ void gpuAtomicAddNoRet(Eigen::half* address,
+                                                Eigen::half val) {
+  atomicAdd(reinterpret_cast<__half*>(address), static_cast<__half>(val));
+}
+#endif
+
+#if defined(USE_ROCM) || ((defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)))
+static inline __device__ void gpuAtomicAddNoRet(Eigen::bfloat16* address,
+                                                Eigen::bfloat16 val) {
+  unsigned int* address_as_ui =
+      (unsigned int*)((char*)address - ((size_t)address & 2));
+  unsigned int old = *address_as_ui;
+  unsigned int assumed;
+  do {
+    assumed = old;
+    uint16_t hsum = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
+    hsum += val;
+
+    old = (size_t)address & 2 ? (old & 0xffff) | (hsum << 16)
+                              : (old & 0xffff0000) | hsum;
+    old = atomicCAS(address_as_ui, assumed, old);
+  } while (assumed != old);
+}
+#else
+static inline __device__ void gpuAtomicAddNoRet(Eigen::bfloat16* address,
+                                                Eigen::bfloat16 val) {
+  atomicAdd(reinterpret_cast<__nv_bfloat16*>(address),
+            static_cast<__nv_bfloat16>(val));
+}
+#endif
+
+/* Used by UpSampleBicubic2d.cu */
+template <typename scalar_t, typename accscalar_t>
+__device__ __forceinline__ static void upsample_increment_value_bounded(
+    scalar_t* data, int batch, int channel, int height, int width, int y, int x,
+    accscalar_t value, int64_t strides0, int64_t strides1, int64_t strides2,
+    int64_t strides3) {
+  int access_y = max(min(y, height - 1), 0);
+  int access_x = max(min(x, width - 1), 0);
+  /* TODO: result here is truncated to scalar_t,
+     check: https://github.com/pytorch/pytorch/pull/19630#discussion_r281426912
+   */
+  const int64_t offset = batch * strides0 + channel * strides1 +
+                         access_y * strides2 + access_x * strides3;
+  gpuAtomicAddNoRet(reinterpret_cast<scalar_t*>(&data[offset]),
+                    static_cast<scalar_t>(value));
+}
+
+// Based on
+// https://en.wikipedia.org/wiki/Bicubic_interpolation#Bicubic_convolution_algorithm
+template <typename accscalar_t>
+__device__ __forceinline__ static accscalar_t cubic_convolution1(
+    accscalar_t x, accscalar_t A) {
+  return ((A + 2) * x - (A + 3)) * x * x + 1;
+}
+
+template <typename accscalar_t>
+__device__ __forceinline__ static accscalar_t cubic_convolution2(
+    accscalar_t x, accscalar_t A) {
+  return ((A * x - 5 * A) * x + 8 * A) * x - 4 * A;
+}
+
+template <typename accscalar_t>
+__device__ __forceinline__ static void get_cubic_upsampling_coefficients(
+    accscalar_t coeffs[4], accscalar_t t) {
+  accscalar_t A = -0.75;
+
+  accscalar_t x1 = t;
+  coeffs[0] = cubic_convolution2<accscalar_t>(x1 + 1.0, A);
+  coeffs[1] = cubic_convolution1<accscalar_t>(x1, A);
+
+  // opposite coefficients
+  accscalar_t x2 = 1.0 - t;
+  coeffs[2] = cubic_convolution1<accscalar_t>(x2, A);
+  coeffs[3] = cubic_convolution2<accscalar_t>(x2 + 1.0, A);
+}
+
+template <typename scalar_t, typename accscalar_t>
+__device__ __forceinline__ static accscalar_t cubic_interp1d(
+    scalar_t x0, scalar_t x1, scalar_t x2, scalar_t x3, accscalar_t t) {
+  accscalar_t coeffs[4];
+  get_cubic_upsampling_coefficients<accscalar_t>(coeffs, t);
+
+  return x0 * coeffs[0] + x1 * coeffs[1] + x2 * coeffs[2] + x3 * coeffs[3];
+}
+
+template <typename scalar_t, typename accscalar_t>
+C10_LAUNCH_BOUNDS_1(1024)
+__global__ void upsample_bicubic2d_out_frame(
+    const int num_elements, const int batchsize, const int channels,
+    const int input_height, const int input_width, const int output_height,
+    const int output_width, const accscalar_t height_scale,
+    const accscalar_t width_scale, const bool align_corners, scalar_t* idata,
+    scalar_t* odata, const int64_t istrides0, const int64_t istrides1,
+    const int64_t istrides2, const int64_t istrides3, const int64_t ostrides0,
+    const int64_t ostrides1, const int64_t ostrides2, const int64_t ostrides3) {
+  int index = threadIdx.x + blockIdx.x * blockDim.x;
+  if (index >= num_elements) {
+    return;
+  }
+  // Special case: input and output are the same size, just copy
+  const int output_x = index % output_width;
+  const int output_y = index / output_width;
+  if (input_height == output_height && input_width == output_width) {
+    for (int n = 0; n < batchsize; n++) {
+      for (int c = 0; c < channels; c++) {
+        int64_t offset = istrides0 * n + istrides1 * c + istrides2 * output_y +
+                         istrides3 * output_x;
+        idata[offset] = odata[offset];
+      }
+    }
+    return;
+  }
+  // Interpolation kernel
+  accscalar_t real_x = area_pixel_compute_source_index(
+      width_scale, output_x, align_corners, /*cubic=*/true);
+  int in_x = floorf(real_x);
+  accscalar_t t_x = real_x - in_x;
+
+  accscalar_t real_y = area_pixel_compute_source_index(
+      height_scale, output_y, align_corners, /*cubic=*/true);
+  int in_y = floorf(real_y);
+  accscalar_t t_y = real_y - in_y;
+
+  for (int n = 0; n < batchsize; n++) {
+    for (int c = 0; c < channels; c++) {
+      accscalar_t coefficients[4];
+
+      for (int k = 0; k < 4; k++) {
+        coefficients[k] = cubic_interp1d(
+            upsample_get_value_bounded<scalar_t>(
+                idata, n, c, input_height, input_width, in_y - 1 + k, in_x - 1,
+                istrides0, istrides1, istrides2, istrides3),
+            upsample_get_value_bounded<scalar_t>(
+                idata, n, c, input_height, input_width, in_y - 1 + k, in_x + 0,
+                istrides0, istrides1, istrides2, istrides3),
+            upsample_get_value_bounded<scalar_t>(
+                idata, n, c, input_height, input_width, in_y - 1 + k, in_x + 1,
+                istrides0, istrides1, istrides2, istrides3),
+            upsample_get_value_bounded<scalar_t>(
+                idata, n, c, input_height, input_width, in_y - 1 + k, in_x + 2,
+                istrides0, istrides1, istrides2, istrides3),
+            t_x);
+      }
+      int64_t offset = ostrides0 * n + ostrides1 * c + ostrides2 * output_y +
+                       ostrides3 * output_x;
+      odata[offset] = static_cast<scalar_t>(
+          cubic_interp1d(coefficients[0], coefficients[1], coefficients[2],
+                         coefficients[3], t_y));
+    }
+  }
+}
+
+template <typename scalar_t, typename accscalar_t>
+C10_LAUNCH_BOUNDS_1(1024)
+__global__ void upsample_bicubic2d_backward_out_frame(
+    const int num_elements, const int batchsize, const int channels,
+    const int input_height, const int input_width, const int output_height,
+    const int output_width, const accscalar_t height_scale,
+    const accscalar_t width_scale, const bool align_corners, scalar_t* grad_idata,
+    scalar_t* grad_odata, const int64_t istrides0, const int64_t istrides1,
+    const int64_t istrides2, const int64_t istrides3, const int64_t ostrides0,
+    const int64_t ostrides1, const int64_t ostrides2, const int64_t ostrides3) {
+  int index = threadIdx.x + blockIdx.x * blockDim.x;
+  if (index >= num_elements) {
+    return;
+  }
+
+  const int output_x = index % output_width;
+  const int output_y = index / output_width;
+  // special case: output_xust copy
+  if (input_height == output_height && input_width == output_width) {
+    for (int n = 0; n < batchsize; n++) {
+      for (int c = 0; c < channels; ++c) {
+        int64_t offset = istrides0 * n + istrides1 * c + istrides2 * output_y +
+                         istrides3 * output_x;
+        grad_idata[offset] = grad_odata[offset];
+      }
+    }
+    return;
+  }
+
+  accscalar_t real_x = area_pixel_compute_source_index(
+      width_scale, output_x, align_corners, /*cubic=*/true);
+  int input_x = floorf(real_x);
+  accscalar_t t_x = real_x - input_x;
+
+  accscalar_t real_y = area_pixel_compute_source_index(
+      height_scale, output_y, align_corners, /*cubic=*/true);
+  int input_y = floorf(real_y);
+  accscalar_t t_y = real_y - input_y;
+  
+  accscalar_t x_coeffs[4];
+  accscalar_t y_coeffs[4];
+
+  get_cubic_upsampling_coefficients(x_coeffs, t_x);
+  get_cubic_upsampling_coefficients(y_coeffs, t_y);
+
+  for (int n = 0; n < batchsize; n++) {
+    for (int c = 0; c < channels; ++c) {
+      int64_t offset = ostrides0 * n + ostrides1 * c + ostrides2 * output_y +
+                       ostrides3 * output_x;
+      scalar_t out_value = grad_odata[offset];
+      for (int i = 0; i < 4; i++) {
+        for (int j = 0; j < 4; j++) {
+          upsample_increment_value_bounded<scalar_t, accscalar_t>(
+              grad_idata, n, c, input_height, input_width, input_y - 1 + i,
+              input_x - 1 + j, out_value * y_coeffs[i] * x_coeffs[j], istrides0,
+              istrides1, istrides2, istrides3);
+        }
+      }
+    }
+  }
+}
+
+}  // namespace
+
+template <typename scalar_t>
+void* GetResizeBicubicKernel() {
+  return reinterpret_cast<void*>(
+      &upsample_bicubic2d_out_frame<scalar_t, float>);
+}
+
+template <typename scalar_t>
+void* GetResizeBicubicGradKernel() {
+  return reinterpret_cast<void*>(
+      &upsample_bicubic2d_backward_out_frame<scalar_t, float>);
+}
+
+
+template void* GetResizeBicubicKernel<Eigen::half>();
+template void* GetResizeBicubicKernel<Eigen::bfloat16>();
+template void* GetResizeBicubicKernel<float>();
+
+template void* GetResizeBicubicGradKernel<Eigen::half>();
+template void* GetResizeBicubicGradKernel<Eigen::bfloat16>();
+template void* GetResizeBicubicGradKernel<float>();
+
+}  // namespace xla::gpu
diff --git a/xla/service/gpu/runtime/resize_bicubic_kernel.h b/xla/service/gpu/runtime/resize_bicubic_kernel.h
new file mode 100644
index 000000000..36211dbcd
--- /dev/null
+++ b/xla/service/gpu/runtime/resize_bicubic_kernel.h
@@ -0,0 +1,53 @@
+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_KERNEL_H_
+#define XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_KERNEL_H_
+
+#include <stddef.h>
+#include <stdint.h>
+
+#include "absl/status/status.h"
+#include "xla/runtime/memref_view.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/gpu/gpu_types.h"
+#include "xla/stream_executor/platform.h"
+#include "xla/xla_data.pb.h"
+
+namespace xla::gpu {
+
+// Input:
+//  - input: [batch_size, channels, input_height, input_width] dtype
+// Output:
+//  - output: [batch_size, channels, output_height, output_width] dtype
+absl::Status RunResizeBicubicImpl(::tensorflow::se::gpu::GpuStreamHandle stream,
+                                  int threads_per_block_limit,
+                                  xla::runtime::StridedMemrefView input,
+                                  xla::runtime::StridedMemrefView output,
+                                  bool align_corners, float scales_h,
+                                  float scales_w);
+// Input:
+// - input: [batch_size, channels, output_height, output_width] dtype
+// Output:
+// - output: [batch_size, channels, input_height, input_width] dtype
+absl::Status RunResizeBicubicGradImpl(
+    ::tensorflow::se::gpu::GpuStreamHandle stream, int threads_per_block_limit,
+    xla::runtime::StridedMemrefView grad_input,
+    xla::runtime::StridedMemrefView grad_output, bool align_corners,
+    float scales_h, float scales_w);
+
+}  // namespace xla::gpu
+
+#endif  // XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_KERNEL_H_
diff --git a/xla/service/gpu/runtime/resize_bicubic_kernel_common.h b/xla/service/gpu/runtime/resize_bicubic_kernel_common.h
new file mode 100644
index 000000000..22f444290
--- /dev/null
+++ b/xla/service/gpu/runtime/resize_bicubic_kernel_common.h
@@ -0,0 +1,36 @@
+/* Copyright 2023 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_KERNEL_COMMON_H_
+#define XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_KERNEL_COMMON_H_
+
+// Contains shared declarations between resize_bicubic_kernel.cc and resize_bicubic_kernel.cu.cc
+// but avoids including ABSL, etc. which some CUDA compilers cannot
+// handle.
+
+namespace xla::gpu {
+
+constexpr uint32_t CUDA_MAX_THREADS_PER_BLOCK = 1024;
+constexpr uint32_t CUDA_THREADS_PER_BLOCK_FALLBACK = 256;
+
+template <typename scalar_t>
+void* GetResizeBicubicKernel();
+
+template <typename scalar_t>
+void* GetResizeBicubicGradKernel();
+
+}  // namespace xla::gpu
+
+#endif  // XLA_SERVICE_GPU_RUNTIME_RESIZE_BICUBIC_KERNEL_COMMON_H_
diff --git a/xla/xla.proto b/xla/xla.proto
index 739960065..5474160b5 100644
--- a/xla/xla.proto
+++ b/xla/xla.proto
@@ -348,7 +348,7 @@ message DebugOptions {
 
   // Per-heap size constraint. New heaps will be created if per-heap max size is
   // reached.
-  int32 xla_multiheap_size_constraint_per_heap = 142;
+  int64 xla_multiheap_size_constraint_per_heap = 142;
 
   reserved 143;  // Was xla_detailed_logging_and_dumping
 
